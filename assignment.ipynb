{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Word2Vec\n",
    "\n",
    "In this assignment, we will see how we can use Word2Vec (or any similar word embedding) to use information from unlabelled data to help us classify better!\n",
    "\n",
    "You will be using the sentiment data from last week, either the yelps or movies, whichever you wish. \n",
    "\n",
    "Your goal will be to simulate the following situation: you have a **small** set of labelled data and a large set of unlabelled data. Show how the two follow 2 techniques compare as the amount of labelled data increases. You should train them on the small labelled subset and test their performance on the rest of the data. \n",
    "\n",
    "In other words, train on 1k, test on 99k. Then train on 2k, test on 98k. Then train on 4k, test on 96k. Etc.\n",
    "\n",
    "1. Logistic regression trained on labelled data, documents represented as term-frequency matrix of your choice. You can learn the vocabulary from the entire dataset or only the labelled data.\n",
    "\n",
    "2. Logistic regression trained on the labelled data, documents represented as word2vec vectors where you train word2vec using the entire dataset. Play around with different settings of word2vec (training window size, K-negative, skip-gram vs BOW, training windows, etc.). Note: we didn't go over the options in detail in class, so you will need to read about them a bit!\n",
    "\n",
    "You can read about the gensime word2vec implementation [here](https://radimrehurek.com/gensim/models/word2vec.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T19:05:32.164853Z",
     "start_time": "2020-05-17T19:05:27.892624Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import datetime as dt\n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T19:05:32.180241Z",
     "start_time": "2020-05-17T19:05:32.167078Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T19:05:32.195838Z",
     "start_time": "2020-05-17T19:05:32.182932Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    doc = re.sub(not_alphanumeric_or_space, '', doc)\n",
    "    words = [t.lemma_ for t in nlp(doc) if t.lemma_ != '-PRON-']\n",
    "    return ' '.join(words).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T19:05:32.209471Z",
     "start_time": "2020-05-17T19:05:32.203974Z"
    }
   },
   "outputs": [],
   "source": [
    "def pca_loadings(v):\n",
    "    pca = TruncatedSVD(2)\n",
    "    pca.fit(v)\n",
    "    return pca.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T19:05:32.218989Z",
     "start_time": "2020-05-17T19:05:32.211852Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_vocab(vector):\n",
    "    text_str = vector.str.cat(sep=' ').lower()\n",
    "    text_list = text_str.split()\n",
    "    text_unique = list(set(text_list))\n",
    "    return text_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T19:05:32.226026Z",
     "start_time": "2020-05-17T19:05:32.221566Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T19:05:32.239937Z",
     "start_time": "2020-05-17T19:05:32.228233Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(V, y, components, epochs):\n",
    "    try:\n",
    "        V = V.toarray()\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    Y = torch.tensor(y, dtype=torch.float32)\n",
    "    X = torch.tensor(V, dtype=torch.float32)\n",
    "\n",
    "    C = torch.tensor(components, dtype=torch.float32, requires_grad=True)\n",
    "    beta = torch.randn((components.shape[0], 1), requires_grad=True)\n",
    "\n",
    "    opt = torch.optim.Adam([C, beta], lr=0.01)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    for i in range(epochs):\n",
    "        L = torch.mm(X, C.T)\n",
    "        out = torch.mm(L, beta)\n",
    "        p = torch.sigmoid(out)\n",
    "        loss = criterion(p, Y)\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            print(f\"[ {timestamp()} ] :: Epoch {i:02} :: {loss.item():0.6f}\")\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return C.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T19:05:32.252688Z",
     "start_time": "2020-05-17T19:05:32.243389Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(V, y, components):\n",
    "    try:\n",
    "        V = V.toarray()\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    Y = torch.tensor(y, dtype=torch.float32)\n",
    "    X = torch.tensor(V, dtype=torch.float32)\n",
    "\n",
    "    C = torch.tensor(components, dtype=torch.float32, requires_grad=True)\n",
    "    beta = torch.randn((components.shape[0], 1), requires_grad=True)\n",
    "\n",
    "    #opt = torch.optim.Adam([C, beta], lr=0.01)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    \n",
    "    L = torch.mm(X, C.T)\n",
    "    out = torch.mm(L, beta)\n",
    "    p = torch.sigmoid(out)\n",
    "    loss = criterion(p, Y)\n",
    "    print(f\"[ {timestamp()} ] :: Test loss: {loss.item():0.6f}\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T19:05:32.259307Z",
     "start_time": "2020-05-17T19:05:32.255294Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_X(X):\n",
    "    return X.apply(lambda x: word_tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T19:05:32.266495Z",
     "start_time": "2020-05-17T19:05:32.261631Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_word_vectors(X):\n",
    "    X_tokens = tokenize_X(X)\n",
    "    model_cbow = gensim.models.Word2Vec(list(X_tokens.values),\n",
    "                                        min_count = 1,  \n",
    "                                        size = 100,\n",
    "                                        window = 5)\n",
    "    return model_cbow.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T19:05:32.272836Z",
     "start_time": "2020-05-17T19:05:32.268699Z"
    }
   },
   "outputs": [],
   "source": [
    "def timestamp():\n",
    "    return dt.datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T19:05:32.282514Z",
     "start_time": "2020-05-17T19:05:32.275178Z"
    }
   },
   "outputs": [],
   "source": [
    "def map_to_wordvec(token_line, word_vectors):\n",
    "    return np.array(np.sum(np.array([word_vectors[token] for token in token_line]), axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T19:05:32.293190Z",
     "start_time": "2020-05-17T19:05:32.285459Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_data(X):\n",
    "    X_tokens = tokenize_X(X)\n",
    "    X_wv = X_tokens.apply(map_to_wordvec, args=(word_vec,))\n",
    "    return X_wv.apply(pd.Series).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-17T19:05:34.910665Z",
     "start_time": "2020-05-17T19:05:32.300992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2020/05/17 21:05:34 ] :: Loading from file...\n"
     ]
    }
   ],
   "source": [
    "sample_fraction = 0.3\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "not_alphanumeric_or_space = re.compile(r'[^(\\w|\\s|\\d)]')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(f\"[ {timestamp()} ] :: Loading from file...\")\n",
    "yelps = pd.read_csv('sentiment/yelps.csv').sample(frac=sample_fraction)\n",
    "# There are three extra rows that are labelled positive, I dropped them\n",
    "yelps = yelps.loc[yelps.positive!='positive'] \n",
    "yelps = yelps[['positive', 'text']]\n",
    "\n",
    "X = yelps['text']\n",
    "# Beacause of the extra rows, the positive column is a string,\n",
    "# so I map it back to an int\n",
    "y = yelps['positive'].map({'True': 1, 'False': 0}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-17T19:05:30.912Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2020/05/17 21:05:36 ] :: Creating vectoriser...\n",
      "*---*---*---*---*---*---*---*---*---*---\n",
      "[ 2020/05/17 21:05:36 ] :: Train size: 0.01\n",
      "[ 2020/05/17 21:05:36 ] :: Fitting transform for training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/r_env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2020/05/17 21:05:49 ] :: Training...\n",
      "[ 2020/05/17 21:05:49 ] :: Epoch 00 :: 3.421689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/r_env/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([299])) that is different to the input size (torch.Size([299, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2020/05/17 21:05:50 ] :: Epoch 20 :: 0.003306\n",
      "[ 2020/05/17 21:05:51 ] :: Epoch 40 :: 0.000922\n",
      "[ 2020/05/17 21:05:52 ] :: Epoch 60 :: 0.000602\n",
      "[ 2020/05/17 21:05:53 ] :: Epoch 80 :: 0.000479\n",
      "[ 2020/05/17 21:05:54 ] :: Fitting transform for testing...\n",
      "[ 2020/05/17 21:18:32 ] :: Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/r_env/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([29700])) that is different to the input size (torch.Size([29700, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2020/05/17 21:19:42 ] :: Test loss: 0.815917\n",
      "\n",
      "\n",
      "*---*---*---*---*---*---*---*---*---*---\n",
      "[ 2020/05/17 21:19:42 ] :: Train size: 0.1\n",
      "[ 2020/05/17 21:19:43 ] :: Fitting transform for training...\n",
      "[ 2020/05/17 21:21:12 ] :: Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/r_env/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([2999])) that is different to the input size (torch.Size([2999, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2020/05/17 21:21:13 ] :: Epoch 00 :: 2.877246\n",
      "[ 2020/05/17 21:21:20 ] :: Epoch 20 :: 0.029786\n",
      "[ 2020/05/17 21:21:27 ] :: Epoch 40 :: 0.016018\n",
      "[ 2020/05/17 21:21:34 ] :: Epoch 60 :: 0.013308\n",
      "[ 2020/05/17 21:21:42 ] :: Epoch 80 :: 0.012202\n",
      "[ 2020/05/17 21:21:49 ] :: Fitting transform for testing...\n",
      "[ 2020/05/17 21:33:07 ] :: Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/r_env/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([27000])) that is different to the input size (torch.Size([27000, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2020/05/17 21:34:24 ] :: Test loss: 13.304457\n",
      "\n",
      "\n",
      "*---*---*---*---*---*---*---*---*---*---\n",
      "[ 2020/05/17 21:34:24 ] :: Train size: 0.5\n",
      "[ 2020/05/17 21:34:25 ] :: Fitting transform for training...\n",
      "[ 2020/05/17 21:50:06 ] :: Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/r_env/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([14999])) that is different to the input size (torch.Size([14999, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2020/05/17 21:50:57 ] :: Epoch 00 :: 5.467865\n",
      "[ 2020/05/17 21:55:51 ] :: Epoch 20 :: 0.071873\n",
      "[ 2020/05/17 22:00:38 ] :: Epoch 40 :: 0.035030\n",
      "[ 2020/05/17 22:05:43 ] :: Epoch 60 :: 0.023990\n",
      "[ 2020/05/17 22:10:30 ] :: Epoch 80 :: 0.018680\n",
      "[ 2020/05/17 22:15:50 ] :: Fitting transform for testing...\n",
      "[ 2020/05/17 22:24:48 ] :: Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/r_env/lib/python3.7/site-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([15000])) that is different to the input size (torch.Size([15000, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2020/05/17 22:25:38 ] :: Test loss: 0.656121\n",
      "\n",
      "\n",
      "*---*---*---*---*---*---*---*---*---*---\n",
      "[ 2020/05/17 22:25:38 ] :: Train size: 0.8\n",
      "[ 2020/05/17 22:25:40 ] :: Fitting transform for training...\n"
     ]
    }
   ],
   "source": [
    "vocab = generate_vocab(X)\n",
    "\n",
    "print(f\"[ {timestamp()} ] :: Creating vectoriser...\")\n",
    "vectorizer = TfidfVectorizer(min_df=2,\n",
    "                             max_df=.8,\n",
    "                             preprocessor=preprocess,\n",
    "                             stop_words='english',\n",
    "                             vocabulary = vocab,\n",
    "                             use_idf=True,\n",
    "                             norm=False)\n",
    "\n",
    "\n",
    "reviews = list(X.apply(lambda x: x.split()).values)\n",
    "\n",
    "mult = [1,10,50,80]\n",
    "train_size_base = 0.01\n",
    "epochs = 100\n",
    "\n",
    "losses = []\n",
    "for m in mult:\n",
    "    train_size = train_size_base * m\n",
    "    print(f\"*---\"*10)\n",
    "    print(f\"[ {timestamp()} ] :: Train size: {train_size}\")\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X,y,train_size = train_size)\n",
    "\n",
    "    print(f\"[ {timestamp()} ] :: Fitting transform for training...\")\n",
    "    v_train = vectorizer.fit_transform(X_train)\n",
    "    v_train = np.asarray(v_train.todense())\n",
    "    word_vecs = pca_loadings(v_train)\n",
    "\n",
    "    print(f\"[ {timestamp()} ] :: Training...\")\n",
    "    learned_vecs = train(v_train, y_train, word_vecs.T, epochs)\n",
    "\n",
    "    print(f\"[ {timestamp()} ] :: Fitting transform for testing...\")\n",
    "    v_test = vectorizer.fit_transform(X_test)\n",
    "    v_test = np.asarray(v_test.todense())\n",
    "\n",
    "    print(f\"[ {timestamp()} ] :: Testing...\")\n",
    "    loss = test(v_test, y_test, learned_vecs)\n",
    "\n",
    "    losses.append({\"train_size\" : train_size, \"loss\": loss.item()})\n",
    "    print(f\"\\n\")\n",
    "    #print(f\"*---\"*10)\n",
    "tfid_loss = pd.DataFrame(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-17T19:05:31.262Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"[ {timestamp()} ] :: Generating Word Vectors\")\n",
    "word_vec = generate_word_vectors(X)\n",
    "\n",
    "mult = [1,10,50,80]\n",
    "train_size_base = 0.01\n",
    "\n",
    "w2v_losses = []\n",
    "for m in mult:\n",
    "    train_size = train_size_base * m\n",
    "    print(f\"*---\"*10)\n",
    "    print(f\"[ {timestamp()} ] :: Train size: {train_size}\")\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X,y,train_size = train_size)\n",
    "\n",
    "    print(f\"[ {timestamp()} ] :: Regressing...\")\n",
    "    lr = LogisticRegression(random_state=0,\n",
    "                            solver='lbfgs', \n",
    "                            max_iter=10000)\n",
    "    clf = lr.fit(encode_data(X_train), y_train)\n",
    "\n",
    "    y_pred = clf.predict(encode_data(X_test))\n",
    "    loss = sklearn.metrics.log_loss(y_test, y_pred)\n",
    "\n",
    "    w2v_losses.append({\"train_size\" : train_size, \"loss\": loss.item()})\n",
    "w2v_loss = pd.DataFrame(w2v_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-17T19:05:31.994Z"
    }
   },
   "outputs": [],
   "source": [
    "tfid_loss.columns = ['train_size', 'tfid_loss']\n",
    "w2v_loss.columns = ['train_size', 'w2v_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-17T19:05:32.331Z"
    }
   },
   "outputs": [],
   "source": [
    "combined = tfid_loss.merge(w2v_loss, on='train_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-17T19:05:32.782Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(combined['train_size'], combined['tfid_loss'], label='TFID')\n",
    "plt.plot(combined['train_size'], combined['w2v_loss'], label='Word2Vec')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "assignment.ipynb",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
